{"version":"1","records":[{"hierarchy":{"lvl1":"Local authority methodology"},"type":"lvl1","url":"/la-methodology","position":0},{"hierarchy":{"lvl1":"Local authority methodology"},"content":"","type":"content","url":"/la-methodology","position":1},{"hierarchy":{"lvl1":"Local authority methodology","lvl2":"Introduction"},"type":"lvl2","url":"/la-methodology#introduction","position":2},{"hierarchy":{"lvl1":"Local authority methodology","lvl2":"Introduction"},"content":"When policy changes in the UK - taxes, benefits, or public spending - it affects places and people differently. PolicyEngine UK builds tools to analyze incomes, jobs, and population patterns in each local authoirity district. This documentation explains how we create a microsimulation model that works at the local authoirity level. The system combines workplace surveys of jobs and earnings, HMRC tax records, and population statistics. We estimate income distributions, and optimize geographic weights.\n\nThis guide shows how to use PolicyEngine UK for local authoirity analysis. We start with data collection, transform it for modeling, and build tools to examine policies. The guide provides examples and code to implement these methods. Users can measure changes in household budgets, track employment, and understand economic patterns on different local authoirity districts. This document starts with data collection from workplace surveys, tax records, and population counts, then explains how we convert this data into usable forms through income brackets. It concludes with technical details about accuracy measurement and calibration, plus example code for analysis and visualization.\n\n","type":"content","url":"/la-methodology#introduction","position":3},{"hierarchy":{"lvl1":"Local authority methodology","lvl2":"Data"},"type":"lvl2","url":"/la-methodology#data","position":4},{"hierarchy":{"lvl1":"Local authority methodology","lvl2":"Data"},"content":"In this section, we describe three main data sources that form the foundation of our local authority level analysis: earning and jobs data from NOMIS ASHE, income statistics from HMRC, and population age distributions from the House of Commons Library.","type":"content","url":"/la-methodology#data","position":5},{"hierarchy":{"lvl1":"Local authority methodology","lvl3":"Earning and jobs data","lvl2":"Data"},"type":"lvl3","url":"/la-methodology#earning-and-jobs-data","position":6},{"hierarchy":{"lvl1":"Local authority methodology","lvl3":"Earning and jobs data","lvl2":"Data"},"content":"Data is extracted from NOMIS Annual Survey of Hours and Earnings (ASHE) - workplace analysis dataset, containing number of jobs and earnings percentiles for all UK local authority districts from the \n\nNOMIS website. This dataset is stored as \n\nnomis_earning_jobs_data.xlsx. To download the data, follow the variable selection process shown in the image below:\n\n","type":"content","url":"/la-methodology#earning-and-jobs-data","position":7},{"hierarchy":{"lvl1":"Local authority methodology","lvl3":"Income data","lvl2":"Data"},"type":"lvl3","url":"/la-methodology#income-data","position":8},{"hierarchy":{"lvl1":"Local authority methodology","lvl3":"Income data","lvl2":"Data"},"content":"Income data for UK local authorities is obtained from \n\nHMRC. This dataset provides detailed information about income and tax by local authorities with confidence intervals, and is stored as \n\ntotal_income.csv, including two key variables:\n\ntotal_income_count: the total number of taxpayers in each local authority\n\ntotal_income_amount: the total amount of income for all taxpayers in each local authority\n\nWe use these measures to identify similar local authorities when employment distribution data is missing. Our approach assumes that local authorities with similar income patterns (measured by both taxpayer counts and total income) will have similar earnings distributions. The following table shows the dataset:\n\nimport pandas as pd\nfrom itables import init_notebook_mode, show\nimport itables.options as opt\nopt.maxBytes = \"1MB\"\ninit_notebook_mode(all_interactive=True)\n\npd.read_csv(\"../policyengine_uk_data/datasets/frs/local_areas/local_authorities/targets/total_income.csv\")\n\n","type":"content","url":"/la-methodology#income-data","position":9},{"hierarchy":{"lvl1":"Local authority methodology","lvl3":"Population data by age","lvl2":"Data"},"type":"lvl3","url":"/la-methodology#population-data-by-age","position":10},{"hierarchy":{"lvl1":"Local authority methodology","lvl3":"Population data by age","lvl2":"Data"},"content":"Population data by age groups for UK local authorities can be downloaded from the \n\nONS data dashboard. The dataset provides detailed age breakdowns for each UK local authority, containing population counts for every age from 0 to 90+ years old across all local authorities in England, Wales, Northern Ireland, and Scotland. The data is stored as \n\nage.csv. The following table shows the dataset:\n\nimport pandas as pd\nfrom itables import init_notebook_mode, show\nimport itables.options as opt\nopt.maxBytes = \"1MB\"\ninit_notebook_mode(all_interactive=True)\n\npd.read_csv(\"../policyengine_uk_data/datasets/frs/local_areas/local_authorities/targets/age.csv\")\n\n","type":"content","url":"/la-methodology#population-data-by-age","position":11},{"hierarchy":{"lvl1":"Local authority methodology","lvl2":"Preprocessing"},"type":"lvl2","url":"/la-methodology#preprocessing","position":12},{"hierarchy":{"lvl1":"Local authority methodology","lvl2":"Preprocessing"},"content":"In this section, we detail a preprocessing step necessary for our local authority level analysis: converting earnings percentiles into practical income brackets.","type":"content","url":"/la-methodology#preprocessing","position":13},{"hierarchy":{"lvl1":"Local authority methodology","lvl3":"Convert earning percentiles to brackets","lvl2":"Preprocessing"},"type":"lvl3","url":"/la-methodology#convert-earning-percentiles-to-brackets","position":14},{"hierarchy":{"lvl1":"Local authority methodology","lvl3":"Convert earning percentiles to brackets","lvl2":"Preprocessing"},"content":"To analyze earnings data effectively, we convert earning percentiles into earning brackets through the following process:\n\nFirst, we estimate the full distribution of earnings by:\n\nUsing known percentile data (10th to 90th) from the ASHE dataset\n\nExtending this to estimate the 90th-99th percentiles using ratios derived from \n\nthis government statistics report\n\nThis estimation allows us to map earnings data into brackets that align with policy thresholds.\n\nAfter estimating the full earnings distribution, we convert the data into income brackets. We calculate the number of jobs and total earnings for each local authority and income bracket based on the estimated earnings distribution. When we encounter local authorities with missing data, we estimate their earnings distribution pattern using data from local authorities with similar total number of taxpayers and total income levels.\n\nThe script \n\ncreate_employment_incomes.py generates \n\nemployment_income.csv containing number of jobs (\n\nemployment_income_count) and total earnings (\n\nemployment_income_amount) for each local authority and income bracket. The following table shows employment and income across different brackets for local authorities:\n\nimport pandas as pd\nfrom itables import init_notebook_mode, show\nimport itables.options as opt\nopt.maxBytes = \"1MB\"\ninit_notebook_mode(all_interactive=True)\n\npd.read_csv(\"../policyengine_uk_data/datasets/frs/local_areas/local_authorities/targets/employment_income.csv\")\n\n","type":"content","url":"/la-methodology#convert-earning-percentiles-to-brackets","position":15},{"hierarchy":{"lvl1":"Local authority methodology","lvl2":"Methodology"},"type":"lvl2","url":"/la-methodology#methodology","position":16},{"hierarchy":{"lvl1":"Local authority methodology","lvl2":"Methodology"},"content":"This section describes our approach to creating accurate local authority level estimates through three key components: a loss function for evaluating accuracy, a calibration process for optimizing weights, and the mathematical framework behind the optimization. To see how well this methodology performs in practice, you can check our detailed \n\nvalidation results page comparing our estimates against actual data at both local authority and national levels.","type":"content","url":"/la-methodology#methodology","position":17},{"hierarchy":{"lvl1":"Local authority methodology","lvl3":"Loss function","lvl2":"Methodology"},"type":"lvl3","url":"/la-methodology#loss-function","position":18},{"hierarchy":{"lvl1":"Local authority methodology","lvl3":"Loss function","lvl2":"Methodology"},"content":"The file \n\nloss.py defines a function \n\ncreate_local_authority_target_matrix that creates target matrices for comparing simulated data against actual local authority level data. The following process outlines how the function processes:\n\nTakes three main input parameters: dataset (defaults to \n\nenhanced_frs_2022_23), time_period (defaults to 2025), and an optional reform parameter for policy changes.\n\nReads three files containing real data: \n\nage.csv, \n\ntotal_income.csv, and \n\nemployment_income.csv.\n\nCreates a PolicyEngine Microsimulation object using the specified dataset and reform parameters.\n\nCreates two main matrices: \n\nmatrix for simulated values from PolicyEngine, and \n\ny for actual target values from both HMRC (income data) and ONS (age data).\n\nCalculates total income metrics at the national level, computing both total amounts and counts of people with income.\n\nProcesses age distributions by creating 10-year age bands from 0 to 80, calculating how many people fall into each band.\n\nProcesses both counts and amounts for different income bands between £12,570 and £70,000, excluding people under 16 for employment income.\n\nMaps individual-level results to household level throughout the \n\nsim.map_result() function.\n\nThe function returns both the simulated matrix and the target matrix \n\n(matrix, y) which can be used for comparing the simulation results against actual data.\n\n","type":"content","url":"/la-methodology#loss-function","position":19},{"hierarchy":{"lvl1":"Local authority methodology","lvl3":"Calibration function","lvl2":"Methodology"},"type":"lvl3","url":"/la-methodology#calibration-function","position":20},{"hierarchy":{"lvl1":"Local authority methodology","lvl3":"Calibration function","lvl2":"Methodology"},"content":"The file \n\ncalibrate.py defines a main \n\ncalibrate() function that performs weight calibration for local authority level analysis.\n\nIt imports necessary functions and matrices from other files including \n\ncreate_local_authority_target_matrix, \n\ncreate_national_target_matrix from \n\nloss.py.\n\nSets up initial matrices using the \n\ncreate_local_authority_target_matrix and \n\ncreate_national_target_matrix functions for both local authority and national level data.\n\nCreates a Microsimulation object using the \n\nenhanced_frs_2022_23 dataset.\n\nInitializes weights for 360 local authority districts x 100180 households, starting with the log of household weights divided by local authority district count.\n\nConverts all the matrices and weights into PyTorch tensors to enable optimization.\n\nDefines a loss function that calculates and combines both local authority level and national-level mean squared errors into a single loss value.\n\nUses Adam optimizer with a learning rate of 0.1 to minimize the loss over 512 epochs.\n\nEvery 100 epochs during optimization, it updates the weights and saves the current weights to a weights.h5 file.\n\n","type":"content","url":"/la-methodology#calibration-function","position":21},{"hierarchy":{"lvl1":"Local authority methodology","lvl3":"Optimization mathematics","lvl2":"Methodology"},"type":"lvl3","url":"/la-methodology#optimization-mathematics","position":22},{"hierarchy":{"lvl1":"Local authority methodology","lvl3":"Optimization mathematics","lvl2":"Methodology"},"content":"In this part, we explain the mathematics behind the calibration process that we discussed above. The optimization uses a two-part loss function that balances local authority level and national-level accuracy, combining both local and national targets into a single optimization problem. The mathematical formulation can be expressed as follows:\n\nFor the local authority level component, we have:\n\nA set of households (j) with known characteristics (metrics_j) like income, age, etc.\n\nA set of local authorities (i) with known target values (y_c) from official statistics\n\nWeights in log space (w_{ij}) that we need to optimize for each household in each local authority\n\nUsing these components, we calculate predicted local authority level statistics. For each local authority metric (e.g. total income), the predicted value is:\\text{pred}_c = \\sum_j (\\exp(w_{ij}) \\times \\text{metrics}_j)\n\nwhere \\text{metrics}_j represents the household-level characteristics for that specific metric (e.g. household income). We use exponential of weights to ensure they stay positive.\n\nTo measure how well our predictions match the real local authority data, we calculate the local authority mean squared error:\\text{MSE}_c = \\text{mean}((\\text{pred}_c / (1 + y_c) - 1)^2)\n\nwhere y_c are the actual target values from official statistics for each local authority. We use relative error (dividing by 1 + y_c) to make errors comparable across different scales of metrics.\n\nFor the national component, we need to ensure our local authority level adjustments don’t distort national-level statistics. We aggregate across all local authorities:\\text{pred}_n = \\sum_i (\\sum_j \\exp(w_{ij})) \\times \\text{metrics}_\\text{national}\n\nwith corresponding mean squared error to measure deviation from national targets:\\text{MSE}_n = \\text{mean}((\\text{pred}_n / (1 + y_n) - 1)^2)\n\nThe total loss combines both local authority and national errors:L = \\text{MSE}_c + \\text{MSE}_n\n\nWe initialize the weights using the original household weights from the survey data:w_{\\text{initial}} = \\ln(\\text{household}_{weight}/650)\n\nwhere 650 is the number of local authorities. These weights are then iteratively optimized using the Adam (Adaptive Moment Estimation) optimizer with a learning rate of 0.1. The optimization process runs for 512 epochs, with the weights being updated in each iteration:w_{t+1} = w_t - 0.1 \\times \\nabla L(w_t)\n\nThis formulation ensures that the optimized weights maintain both local local authority at the local authority level and global accuracy for national-level statistics. The Adam optimizer adaptively adjusts the weights to minimize both local authority level and national-level errors simultaneously, providing efficient convergence through adaptive learning rates and momentum. The resulting optimized weights allow us to accurately reweight household survey data to match both local authority level and national statistics to obtain accurate estimates of income distributions, demographics, and policy impacts for each parliamentary local authority while maintaining local authority with national totals.","type":"content","url":"/la-methodology#optimization-mathematics","position":23},{"hierarchy":{"lvl1":"Constituency methodology"},"type":"lvl1","url":"/constituency-methodology","position":0},{"hierarchy":{"lvl1":"Constituency methodology"},"content":"","type":"content","url":"/constituency-methodology","position":1},{"hierarchy":{"lvl1":"Constituency methodology","lvl2":"Introduction"},"type":"lvl2","url":"/constituency-methodology#introduction","position":2},{"hierarchy":{"lvl1":"Constituency methodology","lvl2":"Introduction"},"content":"When policy changes in the UK - taxes, benefits, or public spending - it affects places and people differently. PolicyEngine UK builds tools to analyze incomes, jobs, and population patterns in each constituency. This documentation explains how we create a microsimulation model that works at the constituency level. The system combines workplace surveys of jobs and earnings, HMRC tax records, and population statistics. We map data between 2010 and 2024 constituency boundaries, estimate income distributions, and optimize geographic weights.\n\nThis guide shows how to use PolicyEngine UK for constituency analysis. We start with data collection, transform it for modeling, and build tools to examine policies. The guide provides examples and code to implement these methods. Users can measure changes in household budgets, track employment, and understand economic patterns on different constituencies. This document starts with data collection from workplace surveys, tax records, and population counts, then explains how we convert this data into usable forms through income brackets and boundary mapping. It concludes with technical details about accuracy measurement and calibration, plus example code for analysis and visualization.\n\n","type":"content","url":"/constituency-methodology#introduction","position":3},{"hierarchy":{"lvl1":"Constituency methodology","lvl2":"Data"},"type":"lvl2","url":"/constituency-methodology#data","position":4},{"hierarchy":{"lvl1":"Constituency methodology","lvl2":"Data"},"content":"In this section, we describe three main data sources that form the foundation of our constituency-level analysis: earning and jobs data from NOMIS ASHE, income statistics from HMRC, and population age distributions from the House of Commons Library.","type":"content","url":"/constituency-methodology#data","position":5},{"hierarchy":{"lvl1":"Constituency methodology","lvl3":"Earning and jobs data","lvl2":"Data"},"type":"lvl3","url":"/constituency-methodology#earning-and-jobs-data","position":6},{"hierarchy":{"lvl1":"Constituency methodology","lvl3":"Earning and jobs data","lvl2":"Data"},"content":"Data is extracted from NOMIS Annual Survey of Hours and Earnings (ASHE) - workplace analysis dataset, containing number of jobs and earnings percentiles for all UK parliamentary constituencies from the \n\nNOMIS website. This dataset is stored as \n\nnomis_earning_jobs_data.xlsx. To download the data, follow the variable selection process shown in the image below:\n\n","type":"content","url":"/constituency-methodology#earning-and-jobs-data","position":7},{"hierarchy":{"lvl1":"Constituency methodology","lvl3":"Income data","lvl2":"Data"},"type":"lvl3","url":"/constituency-methodology#income-data","position":8},{"hierarchy":{"lvl1":"Constituency methodology","lvl3":"Income data","lvl2":"Data"},"content":"Income data for UK parliamentary constituencies is obtained from \n\nHMRC. This dataset provides detailed information about income and tax by Parliamentary constituency with confidence intervals, and is stored as \n\ntotal_income.csv, including two key variables:\n\ntotal_income_count: the total number of taxpayers in each constituency\n\ntotal_income_amount: the total amount of income for all taxpayers in each constituency\n\nWe use these measures to identify similar constituencies when employment distribution data is missing. Our approach assumes that constituencies with similar income patterns (measured by both taxpayer counts and total income) will have similar earnings distributions. The following table shows the dataset:\n\nimport pandas as pd\nfrom itables import init_notebook_mode, show\nimport itables.options as opt\nopt.maxBytes = \"1MB\"\ninit_notebook_mode(all_interactive=True)\n\npd.read_csv(\"../policyengine_uk_data/datasets/frs/local_areas/constituencies/targets/total_income.csv\")\n\n","type":"content","url":"/constituency-methodology#income-data","position":9},{"hierarchy":{"lvl1":"Constituency methodology","lvl3":"Population data by age","lvl2":"Data"},"type":"lvl3","url":"/constituency-methodology#population-data-by-age","position":10},{"hierarchy":{"lvl1":"Constituency methodology","lvl3":"Population data by age","lvl2":"Data"},"content":"Population data by age groups for UK parliamentary constituencies can be downloaded from the \n\nHouse of Commons Library data dashboard. The dataset provides detailed age breakdowns for each UK constituency, containing population counts for every age from 0 to 90+ years old across all parliamentary constituencies in England, Wales, Northern Ireland, and Scotland. The data is stored as \n\nage.csv. The following table shows the dataset:\n\nimport pandas as pd\nfrom itables import init_notebook_mode, show\nimport itables.options as opt\nopt.maxBytes = \"1MB\"\ninit_notebook_mode(all_interactive=True)\n\npd.read_csv(\"../policyengine_uk_data/datasets/frs/local_areas/constituencies/targets/age.csv\")\n\n","type":"content","url":"/constituency-methodology#population-data-by-age","position":11},{"hierarchy":{"lvl1":"Constituency methodology","lvl2":"Preprocessing"},"type":"lvl2","url":"/constituency-methodology#preprocessing","position":12},{"hierarchy":{"lvl1":"Constituency methodology","lvl2":"Preprocessing"},"content":"In this section, we detail two key preprocessing steps necessary for our constituency-level analysis: converting earnings percentiles into practical income brackets, and mapping between different constituency boundary definitions (2010 to 2024).","type":"content","url":"/constituency-methodology#preprocessing","position":13},{"hierarchy":{"lvl1":"Constituency methodology","lvl3":"Convert earning percentiles to brackets","lvl2":"Preprocessing"},"type":"lvl3","url":"/constituency-methodology#convert-earning-percentiles-to-brackets","position":14},{"hierarchy":{"lvl1":"Constituency methodology","lvl3":"Convert earning percentiles to brackets","lvl2":"Preprocessing"},"content":"To analyze earnings data effectively, we convert earning percentiles into earning brackets through the following process:\n\nFirst, we estimate the full distribution of earnings by:\n\nUsing known percentile data (10th to 90th) from the ASHE dataset\n\nExtending this to estimate the 90th-99th percentiles using ratios derived from \n\nthis government statistics report\n\nThis estimation allows us to map earnings data into brackets that align with policy thresholds.\n\nThe following code and visualization demonstrate this process using an example constituency:\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Sample data for Darlington\nincome_data = {\n    'parliamentary constituency 2010': ['Darlington'],\n    'constituency_code': ['E14000658'],\n    'Number of jobs': ['31000'],\n    '10 percentile': [13298.0],\n    '20 percentile': [16723.0],\n    '30 percentile': [20778.0],\n    '40 percentile': [23407.0],\n    '50 percentile': [27158.0],\n    '60 percentile': [30471.0],\n    '70 percentile': [33812.0],\n    '80 percentile': [40717.0],\n    '90 percentile': [55762.0],\n    '91 percentile': [58878.0],\n    '92 percentile': [62394.4],\n    '93 percentile': [66722.3],\n    '94 percentile': [71952.0],\n    '95 percentile': [78804.5],\n    '96 percentile': [87640.7],\n    '97 percentile': [100083.5],\n    '98 percentile': [123526.5],\n    '100 percentile': [179429.0]\n}\n\nincome_sample = pd.DataFrame(income_data)\n\n# Excel Data Method\ndef load_real_data():\n    # Read Excel data\n    income_real = pd.read_excel(\"nomis_earning_jobs_data.xlsx\", skiprows=7)\n    income_real.columns = income_real.iloc[0]\n    income_real = income_real.drop(index=0).reset_index(drop=True)\n    \n    # Select and rename columns\n    columns_to_keep = [\n        'parliamentary constituency 2010',\n        'constituency_code',\n        'Number of jobs',\n        'Median',\n        '10 percentile',\n        '20 percentile',\n        '30 percentile',\n        '40 percentile',\n        '60 percentile',\n        '70 percentile',\n        '80 percentile',\n        '90 percentile'\n    ]\n    income_real = income_real[columns_to_keep]\n    income_real = income_real.rename(columns={'Median': '50 percentile'})\n    return income_real\n\n# Plotting function\ndef plot_constituency_distribution(income_df, constituency_name, detailed=True):\n    constituency_data = income_df[income_df['parliamentary constituency 2010'] == constituency_name].iloc[0]\n    \n    percentiles = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 91, 92, 93, 94, 95, 96, 97, 98, 100]\n    income_values = [\n        0,\n        constituency_data['10 percentile'],\n        constituency_data['20 percentile'],\n        constituency_data['30 percentile'],\n        constituency_data['40 percentile'],\n        constituency_data['50 percentile'],\n        constituency_data['60 percentile'],\n        constituency_data['70 percentile'],\n        constituency_data['80 percentile'],\n        constituency_data['90 percentile'],\n        constituency_data['91 percentile'],\n        constituency_data['92 percentile'],\n        constituency_data['93 percentile'],\n        constituency_data['94 percentile'],\n        constituency_data['95 percentile'],\n        constituency_data['96 percentile'],\n        constituency_data['97 percentile'],\n        constituency_data['98 percentile'],\n        constituency_data['100 percentile']\n    ]\n    \n    valid_data = [(p, v) for p, v in zip(percentiles, income_values) if pd.notna(v)]\n    filtered_percentiles, filtered_income = zip(*valid_data)\n    \n    plt.figure(figsize=(8, 6))\n    plt.plot(filtered_percentiles, filtered_income, marker='o')\n    plt.xlabel('Percentiles')\n    plt.ylabel('Income')\n    plt.title(f'Income Distribution for {constituency_name}')\n    plt.grid(True)\n    plt.show()\n\n# Plot sample data (Darlington with detailed percentiles)\nplot_constituency_distribution(income_sample, 'Darlington', detailed=True)\n\n\n\nAfter estimating the full earnings distribution, we convert the data into income brackets. We calculate the number of jobs and total earnings for each constituency and income bracket based on the estimated earnings distribution. When we encounter constituencies with missing data, we estimate their earnings distribution pattern using data from constituencies with similar total number of taxpayers and total income levels.\n\nThe Python script \n\ncreate_employment_incomes.py generates \n\nemployment_income.csv containing number of jobs (\n\nemployment_income_count) and total earnings (\n\nemployment_income_amount) for each constituency and income bracket. The following table shows employment and income across different brackets for constituencies:\n\nimport pandas as pd\nfrom itables import init_notebook_mode, show\nimport itables.options as opt\nopt.maxBytes = \"1MB\"\ninit_notebook_mode(all_interactive=True)\n\npd.read_csv(\"../policyengine_uk_data/datasets/frs/local_areas/constituencies/targets/employment_income.csv\")\n\n","type":"content","url":"/constituency-methodology#convert-earning-percentiles-to-brackets","position":15},{"hierarchy":{"lvl1":"Constituency methodology","lvl3":"Mapping constituencies from 2010 to 2024","lvl2":"Preprocessing"},"type":"lvl3","url":"/constituency-methodology#mapping-constituencies-from-2010-to-2024","position":16},{"hierarchy":{"lvl1":"Constituency methodology","lvl3":"Mapping constituencies from 2010 to 2024","lvl2":"Preprocessing"},"content":"PolicyEngine uses HMRC income data which aligns with 2010 constituency boundaries; to handle this issue and align it with 2024 constituency boundaries definitions, we follow these processes:\n\nDownload the mapping data from the \n\nONS website that contains the official lookup table between 2010 and 2024 Westminster Parliamentary Constituencies.\n\nCreate a mapping matrix (650 x 650) which maps each constituency from 2010 to corresponding constituency in 2024 using the boundary_changes/mapping_matrix.py script. This is a many-to-many mapping, as 2010 constituencies can be split across multiple 2024 constituencies, and 2024 constituencies can contain parts of multiple 2010 constituencies. The matrix structure has rows representing 2010 constituencies and columns representing 2024 constituencies.\n\nFor each row in the matrix (representing a 2010 constituency), normalize the weights so they sum to 1. This ensures that when we redistribute data from 2010 boundaries to 2024 boundaries, we maintain the correct proportions.\n\n","type":"content","url":"/constituency-methodology#mapping-constituencies-from-2010-to-2024","position":17},{"hierarchy":{"lvl1":"Constituency methodology","lvl2":"Methodology"},"type":"lvl2","url":"/constituency-methodology#methodology","position":18},{"hierarchy":{"lvl1":"Constituency methodology","lvl2":"Methodology"},"content":"This section describes our approach to creating accurate constituency-level estimates through three key components: a loss function for evaluating accuracy, a calibration process for optimizing weights, and the mathematical framework behind the optimization. To see how well this methodology performs in practice, you can check our detailed \n\nvalidation results page comparing our estimates against actual data at both constituency and national levels.","type":"content","url":"/constituency-methodology#methodology","position":19},{"hierarchy":{"lvl1":"Constituency methodology","lvl3":"Loss function","lvl2":"Methodology"},"type":"lvl3","url":"/constituency-methodology#loss-function","position":20},{"hierarchy":{"lvl1":"Constituency methodology","lvl3":"Loss function","lvl2":"Methodology"},"content":"The file \n\nloss.py defines a function \n\ncreate_constituency_target_matrix that creates target matrices for comparing simulated data against actual constituency-level data. The following process outlines how the function processes:\n\nTakes three main input parameters: dataset (defaults to \n\nenhanced_frs_2022_23), time_period (defaults to 2025), and an optional reform parameter for policy changes.\n\nReads three files containing real data: \n\nage.csv, \n\ntotal_income.csv, and \n\nemployment_income.csv.\n\nCreates a PolicyEngine Microsimulation object using the specified dataset and reform parameters.\n\nCreates two main matrices: \n\nmatrix for simulated values from PolicyEngine, and \n\ny for actual target values from both HMRC (income data) and ONS (age data).\n\nCalculates total income metrics at the national level, computing both total amounts and counts of people with income.\n\nProcesses age distributions by creating 10-year age bands from 0 to 80, calculating how many people fall into each band.\n\nProcesses both counts and amounts for different income bands between £12,570 and £70,000, excluding people under 16 for employment income.\n\nMaps individual-level results to household level throughout the \n\nsim.map_result() function.\n\nThe function returns both the simulated matrix and the target matrix \n\n(matrix, y) which can be used for comparing the simulation results against actual data.\n\n","type":"content","url":"/constituency-methodology#loss-function","position":21},{"hierarchy":{"lvl1":"Constituency methodology","lvl3":"Calibration function","lvl2":"Methodology"},"type":"lvl3","url":"/constituency-methodology#calibration-function","position":22},{"hierarchy":{"lvl1":"Constituency methodology","lvl3":"Calibration function","lvl2":"Methodology"},"content":"The file \n\ncalibrate.py defines a main \n\ncalibrate() function that performs weight calibration for constituency-level analysis.\n\nIt imports necessary functions and matrices from other files including \n\ncreate_constituency_target_matrix, \n\ncreate_national_target_matrix from \n\nloss.py, and \n\ntransform_2010_to_2024 for constituency boundary transformations.\n\nSets up initial matrices using the \n\ncreate_constituency_target_matrix and \n\ncreate_national_target_matrix functions for both constituency and national level data.\n\nCreates a Microsimulation object using the \n\nenhanced_frs_2022_23 dataset.\n\nInitializes weights for 650 constituencies x 100180 households, starting with the log of household weights divided by constituency count.\n\nConverts all the matrices and weights into PyTorch tensors to enable optimization.\n\nDefines a loss function that calculates and combines both constituency-level and national-level mean squared errors into a single loss value.\n\nUses Adam optimizer with a learning rate of 0.1 to minimize the loss over 512 epochs.\n\nEvery 100 epochs during optimization, it updates the weights using the mapping matrix from 2010 to 2024 constituencies and saves the current weights to a weights.h5 file.\n\nIncludes an \n\nupdate_weights() function that applies the constituency mapping matrix to transform the weights between different boundary definitions.\n\n","type":"content","url":"/constituency-methodology#calibration-function","position":23},{"hierarchy":{"lvl1":"Constituency methodology","lvl3":"Optimization mathematics","lvl2":"Methodology"},"type":"lvl3","url":"/constituency-methodology#optimization-mathematics","position":24},{"hierarchy":{"lvl1":"Constituency methodology","lvl3":"Optimization mathematics","lvl2":"Methodology"},"content":"In this part, we explain the mathematics behind the calibration process that we discussed above. The optimization uses a two-part loss function that balances constituency-level and national-level accuracy, combining both local and national targets into a single optimization problem. The mathematical formulation can be expressed as follows:\n\nFor the constituency-level component, we have:\n\nA set of households (j) with known characteristics (metrics_j) like income, age, etc.\n\nA set of constituencies (i) with known target values (y_c) from official statistics\n\nWeights in log space (w_{ij}) that we need to optimize for each household in each constituency\n\nUsing these components, we calculate predicted constituency-level statistics. For each constituency metric (e.g. total income), the predicted value is:\\text{pred}_c = \\sum_j (\\exp(w_{ij}) \\times \\text{metrics}_j)\n\nwhere \\text{metrics}_j represents the household-level characteristics for that specific metric (e.g. household income). We use exponential of weights to ensure they stay positive.\n\nTo measure how well our predictions match the real constituency data, we calculate the constituency mean squared error:\\text{MSE}_c = \\text{mean}((\\text{pred}_c / (1 + y_c) - 1)^2)\n\nwhere y_c are the actual target values from official statistics for each constituency. We use relative error (dividing by 1 + y_c) to make errors comparable across different scales of metrics.\n\nFor the national component, we need to ensure our constituency-level adjustments don’t distort national-level statistics. We aggregate across all constituencies:\\text{pred}_n = \\sum_i (\\sum_j \\exp(w_{ij})) \\times \\text{metrics}_\\text{national}\n\nwith corresponding mean squared error to measure deviation from national targets:\\text{MSE}_n = \\text{mean}((\\text{pred}_n / (1 + y_n) - 1)^2)\n\nThe total loss combines both constituency and national errors:L = \\text{MSE}_c + \\text{MSE}_n\n\nWe initialize the weights using the original household weights from the survey data:w_{\\text{initial}} = \\ln(\\text{household}_{weight}/650)\n\nwhere 650 is the number of constituencies. These weights are then iteratively optimized using the Adam (Adaptive Moment Estimation) optimizer with a learning rate of 0.1. The optimization process runs for 512 epochs, with the weights being updated in each iteration:w_{t+1} = w_t - 0.1 \\times \\nabla L(w_t)\n\nThis formulation ensures that the optimized weights maintain both local consistency at the constituency level and global accuracy for national-level statistics. The Adam optimizer adaptively adjusts the weights to minimize both constituency-level and national-level errors simultaneously, providing efficient convergence through adaptive learning rates and momentum. The resulting optimized weights allow us to accurately reweight household survey data to match both constituency-level and national statistics to obtain accurate estimates of income distributions, demographics, and policy impacts for each parliamentary constituency while maintaining consistency with national totals.\n\n ![](pictures/parliamentary_earnings.png) ","type":"content","url":"/constituency-methodology#optimization-mathematics","position":25},{"hierarchy":{"lvl1":"Introduction"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Introduction"},"content":"PolicyEngine-UK-Data is a package that creates representative microdata for the UK,\ndesigned for input in the PolicyEngine tax-benefit microsimulation model. This tool\nallows users to explore the data sources, validation processes, and enhancements\nmade to ensure accurate and reliable microsimulation results.\n\nPolicyEngine is a tool with a clear purpose: for given assumptions about UK government policy and UK households, predicting what UK households will look like in the next few years. To do that, we need both of two things:\n\nAn accurate model of the effects of policy rules on households.\n\nAn accurate representation of the current UK household sector now.\n\nThis repository is dedicated to the second of those. In this documentation, we’ll explain how we do that, but we’ll also use our model (the first bullet) to see what we end up with when we combine the two, and measure up against other organisations doing the same thing.","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Methodology"},"type":"lvl1","url":"/methodology","position":0},{"hierarchy":{"lvl1":"Methodology"},"content":"In this page, we’ll walk through step-by-step the process we use to create PolicyEngine’s dataset.\n\nFamily Resources Survey: we’ll start with the FRS, looking at close it is to reality. To take an actual concrete starting point, we’ll assume benefit payments are as reported in the survey.\n\nFRS (+ tax-benefit model): we need to make sure that our tax-benefit model isn’t doing anything unexpected. If we turn on simulation of taxes and benefits, does anything look unexpected? If not- great, we’ve turned a household survey into something useful for policy analysis. We’ll also take stock here of what we’re missing from reality.\n\nWealth and consumption: the most obvious thing we’re missing is wealth and consumption. We’ll impute those here.\n\nFine-tuning: we’ll use reweighting to make some final adjustments to make sure our dataset is as close to reality as possible.\n\nValidation: we’ll compare our dataset to the UK’s official statistics, and see how we’re doing.","type":"content","url":"/methodology","position":1},{"hierarchy":{"lvl1":"Methodology","lvl2":"The Family Resources Survey"},"type":"lvl2","url":"/methodology#the-family-resources-survey","position":2},{"hierarchy":{"lvl1":"Methodology","lvl2":"The Family Resources Survey"},"content":"First, we’ll start with the FRS as-is. Skipping over the technical details for how we actually feed this data into the model (you can find that in policyengine_uk_data/datasets/frs/), we need to decide how we’re actually going to measure ‘close to reality’. We need to define an objective function, and if our final dataset improves it a lot, we can call that a success.\n\nWe’ll define this objective function using public statistics that we can generally agree are of high importance to describing the UK household sector. These are things that, if the survey gets them wrong, we’d expect to cause inaccuracy in our model, and if we get them all mostly right, we’d expect to have confidence that it’s a pretty accurate tax-benefit model.\n\nFor this, we’ve gone through and collected:\n\nDemographics from the ONS: ten-year age band populations by region of the UK, national family type populations and national tenure type populations.\n\nIncomes from HMRC: for each of 14 total income bands, the number of people with income and combined income of the seven income types that account for over 99% of total income: employment, self-employment, State Pension, private pension, property, savings interest, and dividends.\n\nTax-benefit programs from the DWP and OBR: statistics on caseloads, expenditures and revenues for all 20 major tax-benefit programs.\n\nLet’s first take a look at the initial FRS, our starting point, and what is generally considered the best dataset to use (mostly completely un-modified across major tax-benefit models), and see how close it is to reproducing these statistics.\n\nThe table below shows the result, and: it’s really quite bad! Look at the relative errors.\n\nfrom policyengine_uk_data.utils import get_loss_results\nfrom policyengine_uk_data import (\n    FRS_2022_23,\n    ExtendedFRS_2022_23,\n    EnhancedFRS_2022_23,\n    ReweightedFRS_2022_23,\n)\nfrom policyengine_core.model_api import Reform\nimport plotly.express as px\nimport pandas as pd\nfrom itables import init_notebook_mode\nimport itables.options as opt\nopt.maxBytes = \"1MB\"\n\ninit_notebook_mode(all_interactive=True)\n\ndef get_loss(dataset, reform, time_period):\n    loss_results = get_loss_results(dataset, time_period, reform)\n\n    def get_type(name):\n        if \"hmrc\" in name:\n            return \"Income\"\n        if \"ons\" in name:\n            return \"Demographics\"\n        if \"obr\" in name:\n            return \"Tax-benefit\"\n        return \"Other\"\n\n    loss_results[\"type\"] = loss_results.name.apply(get_type)\n    return loss_results\n\nreported_benefits = Reform.from_dict(\n    {\n        \"gov.contrib.policyengine.disable_simulated_benefits\": True,\n    }\n)\nloss_results = get_loss(\n    dataset=FRS_2022_23, reform=reported_benefits, time_period=2025\n).copy()\n\nloss_results\n\nIt’s easier to understand ‘what kind of bad’ this is by splitting out the statistics into those three categories. Here’s a histogram of the absolute relative errors.\n\ndef format_fig(fig):\n    return fig\n\nloss_results.abs_rel_error = loss_results.abs_rel_error.clip(0, 1)\n    \nfig = px.histogram(\n    loss_results,\n    x=\"abs_rel_error\",\n    nbins=25,\n    title=\"Distribution of absolute relative errors\",\n    labels={\n        \"value\": \"Absolute relative error\",\n        \"count\": \"Number of variables\",\n    },\n    color=\"type\",\n    color_discrete_sequence=px.colors.qualitative.T10,\n).update_layout(\n    legend_title=\"Category\",\n    xaxis_title=\"Absolute relative error\",\n    yaxis_title=\"Number of variables\",\n    xaxis_tickformat=\".0%\",)\nformat_fig(fig)\n\nA few notes:\n\nWe’re comparing things in the same relevant time period (2022), and only doing a tiny amount of adjustment to the statistics: OBR statistics are taken directly from the latest EFO, ONS statistics are the most recent projections for 2022, and HMRC statistics are uprated from 2021 to 2022 using the same standard uprating factors we use in the model (and it’s only one year adjustment).\n\nDemogaphics look basically fine: that’s expected, because the DWP applies an optimisation algorithm to optimise the household weights to be as close as possible to a similar set of demographic statistics. It’s a good sign that we use slightly different statistics than it was trained on and get good accuracy.\n\nIncomes look not great at all. We’ll take a closer look below to understand why. But the FRS is well-known to under-report income significantly.\n\nTax-benefit programs also look not good. And this is a concern! Because we’re using this dataset to answer questions about tax-benefit programs, and the FRS isn’t even providing a good representation of them under baseline law.\n\nincomes = loss_results[loss_results.type == \"Income\"]\nincomes[\"band\"] = incomes.name.apply(\n    lambda x: \"_\".join(x.split(\"band_\")[1].split(\"_\")[1:])\n)\nincomes[\"count\"] = incomes.name.apply(lambda x: \"count\" in x)\nincomes[\"variable\"] = incomes.name.apply(\n    lambda x: x.split(\"_income_band\")[0].split(\"_count\")[0].split(\"hmrc/\")[-1]\n)\n\nvariable = \"employment_income\"\ncount = True\nvariable_df = incomes[\n    (incomes.variable == variable) & (incomes[\"count\"] == count)\n]\n\nfig = px.bar(\n    variable_df,\n    x=\"band\",\n    y=[\n        \"target\",\n        \"estimate\",\n        \"error\",\n        \"rel_error\",\n        \"abs_error\",\n        \"abs_rel_error\",\n    ],\n    barmode=\"group\",\n    color_discrete_sequence=px.colors.qualitative.T10,\n)\n\nfig = fig.update_layout(\n    title=\"Estimates and ground truth for employment income band counts\",\n    xaxis_title=\"\",\n    yaxis_title=\"Value\",\n    legend_title=\"Variable\",\n)\nformat_fig(fig)\n\nvariable = \"dividend_income\"\ncount = True\nvariable_df = incomes[\n    (incomes.variable == variable) & (incomes[\"count\"] == count)\n]\n\nfig = px.bar(\n    variable_df,\n    x=\"band\",\n    y=[\n        \"target\",\n        \"estimate\",\n        \"error\",\n        \"rel_error\",\n        \"abs_error\",\n        \"abs_rel_error\",\n    ],\n    barmode=\"group\",\n    color_discrete_sequence=px.colors.qualitative.T10,\n)\n\nfig = fig.update_layout(\n    title=\"Estimates and ground truth for dividend income band counts\",\n    xaxis_title=\"\",\n    yaxis_title=\"Value\",\n    legend_title=\"Variable\",\n)\nformat_fig(fig)\n\nThere are a few interesting things here:\n\nThe FRS over-estimates incomes in the upper-middle of the distribution and under-estimates them in the top of the distribution. The reason for this is probably: the FRS misses out the top completely, and then because of the weight optimisation (which scales up the working-age age groups to hit their population targets), the middle of the distribution is inflated, overcompensating.\n\nSome income types are severely under-estimated across all bands: notably capital incomes. This probably reflects issues with the survey questionnaire design more than sampling bias.\n\nOK, so what can we do about it?","type":"content","url":"/methodology#the-family-resources-survey","position":3},{"hierarchy":{"lvl1":"Methodology","lvl2":"Simulating benefits"},"type":"lvl2","url":"/methodology#simulating-benefits","position":4},{"hierarchy":{"lvl1":"Methodology","lvl2":"Simulating benefits"},"content":"First, let’s turn on the model and check nothing unexpected happens. The table below shows each of our known statistics, and how they changed after replacing reported benefits with simulated benefits.\n\noriginal_frs_loss = loss_results.copy()\nfrs_loss = get_loss(FRS_2022_23, None, 2025).copy()\ncombined_frs_loss = pd.merge(\n    on=\"name\",\n    left=original_frs_loss,\n    right=frs_loss,\n    suffixes=(\"_original\", \"_simulated\"),\n)\ncombined_frs_loss[\"change_in_abs_rel_error\"] = (\n    combined_frs_loss[\"abs_rel_error_simulated\"]\n    - combined_frs_loss[\"abs_rel_error_original\"]\n)\n# Sort columns\ncombined_frs_loss.sort_index(axis=1, inplace=True)\ncombined_frs_loss = combined_frs_loss.set_index(\"name\")\n\ncombined_frs_loss\n\nAgain, a few notes:\n\nYou might be thinking: ‘why do some of the HMRC income statistics change?’. That’s because of the State Pension, which is simulated in the model. The State Pension is a component of total income, so people might be moved from one income band to another if we adjust their State Pension payments slightly.\n\nSome of the tax-benefit statistics change, and get better and worse. This is expected for a variety of reasons- one is that incomes and benefits are often out of sync with each other in the data (the income in the survey week might not match income in the benefits assessment time period).","type":"content","url":"/methodology#simulating-benefits","position":5},{"hierarchy":{"lvl1":"Methodology","lvl2":"Adding imputations"},"type":"lvl2","url":"/methodology#adding-imputations","position":6},{"hierarchy":{"lvl1":"Methodology","lvl2":"Adding imputations"},"content":"Now, let’s add in the imputations for wealth and consumption. For this, we train quantile regression forests (essentially, random forest models that capture the conditional distribution of the data) to predict wealth and consumption variables from FRS-shared variables in other surveys.\n\nThe datasets we use are:\n\nThe Wealth and Assets Survey (WAS) for wealth imputations.\n\nThe Living Costs and Food Survey (LCFS) for most consumption imputations.\n\nThe Effects of Taxes and Benefits on Household Income (ETB) for ‘£ consumption that is full VAT rateable’. For example, different households will have different profiles in terms of the share of their consumption that falls on the VATable items.\n\nBelow is a table showing how just adding these imputations changes our objective statistics (filtered to just rows which changed). Not bad pre-calibrated performance! And we’ve picked up an extra £200bn in taxes.\n\nnew_loss = get_loss(ExtendedFRS_2022_23, None, 2025).copy()\nnew_loss_against_old = pd.merge(\n    on=\"name\",\n    left=frs_loss,\n    right=new_loss,\n    suffixes=(\"_simulated\", \"_imputed\"),\n)\nnew_loss_against_old[\"change_in_abs_rel_error\"] = (\n    new_loss_against_old[\"abs_rel_error_imputed\"]\n    - new_loss_against_old[\"abs_rel_error_simulated\"]\n)\n\nnew_loss_against_old\n\n","type":"content","url":"/methodology#adding-imputations","position":7},{"hierarchy":{"lvl1":"Methodology","lvl2":"Calibration"},"type":"lvl2","url":"/methodology#calibration","position":8},{"hierarchy":{"lvl1":"Methodology","lvl2":"Calibration"},"content":"Now, we’ve got a dataset that’s performs pretty well without explicitly targeting the official statistics we care about. So it’s time to add the final touch- calibrating the weights to explicitly minimise error against the target set.\n\ncalibrated_loss = get_loss(ReweightedFRS_2022_23, None, 2025).copy()\ncalibrated_loss_against_imputed = pd.merge(\n    on=\"name\",\n    left=new_loss,\n    right=calibrated_loss,\n    suffixes=(\"_imputed\", \"_calibrated\"),\n)\n\ncalibrated_loss_against_imputed[\"change_in_abs_rel_error\"] = (\n    calibrated_loss_against_imputed[\"abs_rel_error_calibrated\"]\n    - calibrated_loss_against_imputed[\"abs_rel_error_imputed\"]\n)\ncalibrated_loss_against_imputed\n\nLet’s also look at incomes.\n\nincomes = calibrated_loss[loss_results.type == \"Income\"]\nincomes[\"band\"] = incomes.name.apply(\n    lambda x: \"_\".join(x.split(\"band_\")[1].split(\"_\")[1:])\n)\nincomes[\"count\"] = incomes.name.apply(lambda x: \"count\" in x)\nincomes[\"variable\"] = incomes.name.apply(\n    lambda x: x.split(\"_income_band\")[0].split(\"_count\")[0].split(\"hmrc/\")[-1]\n)\n\nvariable = \"employment_income\"\ncount = True\nvariable_df = incomes[\n    (incomes.variable == variable) & (incomes[\"count\"] == count)\n]\n\nfig = px.bar(\n    variable_df,\n    x=\"band\",\n    y=[\n        \"target\",\n        \"estimate\",\n        \"error\",\n        \"rel_error\",\n        \"abs_error\",\n        \"abs_rel_error\",\n    ],\n    barmode=\"group\",\n    color_discrete_sequence=px.colors.qualitative.T10,\n)\nformat_fig(fig)\n\nSo, what’s happening here seems like: the FRS just doesn’t have enough high-income records for calibration to work straight away. The optimiser can’t just set really high weights for the few rich people we do have, because it’d hurt performance on the demographic statistics.\n\nSo, we need a solution to add more high-income records. What we’ll do is:\n\nTrain a QRF model to predict the distributions of income variables from the Survey of Personal Incomes from FRS demographic variables.\n\nFor each FRS person, add an ‘imputed income’ clone with zero weight.\n\nRun the calibration again.","type":"content","url":"/methodology#calibration","position":9},{"hierarchy":{"lvl1":"Methodology","lvl2":"The Enhanced FRS"},"type":"lvl2","url":"/methodology#the-enhanced-frs","position":10},{"hierarchy":{"lvl1":"Methodology","lvl2":"The Enhanced FRS"},"content":"Let’s see how this new dataset performs.\n\nefrs_loss = get_loss(EnhancedFRS_2022_23, None, 2025).copy()\nefrs_loss_against_calibrated = pd.merge(\n    on=\"name\",\n    left=calibrated_loss,\n    right=efrs_loss,\n    suffixes=(\"_calibrated\", \"_enhanced\"),\n)\nefrs_loss_against_calibrated[\"change_in_abs_rel_error\"] = (\n    efrs_loss_against_calibrated[\"abs_rel_error_enhanced\"]\n    - efrs_loss_against_calibrated[\"abs_rel_error_calibrated\"]\n)\nefrs_loss_against_calibrated\n\nAnd finally, let’s look at those incomes again.\n\nincomes = efrs_loss[loss_results.type == \"Income\"]\nincomes[\"band\"] = incomes.name.apply(\n    lambda x: \"_\".join(x.split(\"band_\")[1].split(\"_\")[1:])\n)\nincomes[\"count\"] = incomes.name.apply(lambda x: \"count\" in x)\nincomes[\"variable\"] = incomes.name.apply(\n    lambda x: x.split(\"_income_band\")[0].split(\"_count\")[0].split(\"hmrc/\")[-1]\n)\n\nvariable = \"employment_income\"\ncount = True\nvariable_df = incomes[\n    (incomes.variable == variable) & (incomes[\"count\"] == count)\n]\n\nfig = px.bar(\n    variable_df,\n    x=\"band\",\n    y=[\n        \"target\",\n        \"estimate\",\n        \"error\",\n        \"rel_error\",\n        \"abs_error\",\n        \"abs_rel_error\",\n    ],\n    barmode=\"group\",\n    color_discrete_sequence=px.colors.qualitative.T10,\n)\nformat_fig(fig)\n\nEverything looks healthy here! We’ve got a dataset that’s close to reality, and we can have confidence in our tax-benefit model.","type":"content","url":"/methodology#the-enhanced-frs","position":11},{"hierarchy":{"lvl1":"Pension contributions"},"type":"lvl1","url":"/pension-contributions","position":0},{"hierarchy":{"lvl1":"Pension contributions"},"content":"We take pension contributions from the FRS data where available. This includes:\n\nEmployee contributions (~£30 billion in FY2022)\n\nPersonal contributions (~£5 billion in FY2022)\n\nWe do not have data on employer contributions, but from aggregate data this is around \n\n250% of employee contributions. We apply this as a multiplier to estimate employer contributions for each worker.","type":"content","url":"/pension-contributions","position":1},{"hierarchy":{"lvl1":"Constituency dataset validation"},"type":"lvl1","url":"/validation/constituencies","position":0},{"hierarchy":{"lvl1":"Constituency dataset validation"},"content":"","type":"content","url":"/validation/constituencies","position":1},{"hierarchy":{"lvl1":"Constituency dataset validation"},"type":"lvl1","url":"/validation/constituencies#constituency-dataset-validation","position":2},{"hierarchy":{"lvl1":"Constituency dataset validation"},"content":"\n\nfrom policyengine_uk import Microsimulation\nimport pandas as pd\nimport h5py\nfrom itables import init_notebook_mode\nimport itables.options as opt\nfrom pathlib import Path\nfrom policyengine_uk_data.storage import STORAGE_FOLDER\n\nopt.maxBytes = \"1MB\"\ninit_notebook_mode(all_interactive=True)\n\nREPO = Path(\".\").resolve().parent\n\nweights_file_path = STORAGE_FOLDER / \"parliamentary_constituency_weights.h5\"\nwith h5py.File(weights_file_path, \"r\") as f:\n        weights = f[str(2025)][...]\n\nconstituencies = pd.read_csv(STORAGE_FOLDER / \"constituencies_2024.csv\")\n\nbaseline = Microsimulation()\nhousehold_weights = baseline.calculate(\"household_weight\", 2025).values\n\nfrom policyengine_uk_data.datasets.frs.local_areas.constituencies.loss import create_constituency_target_matrix, create_national_target_matrix\nfrom policyengine_uk_data.datasets import EnhancedFRS_2022_23\nconstituency_target_matrix, constituency_actuals, _ = create_constituency_target_matrix(EnhancedFRS_2022_23, 2025, None)\nnational_target_matrix, national_actuals = create_national_target_matrix(EnhancedFRS_2022_23, 2025, None)\n\nconstituency_wide = weights @ constituency_target_matrix\nconstituency_wide.index = constituencies.code.values\nconstituency_wide[\"name\"] = constituencies.name.values\n\nconstituency_results = pd.melt(constituency_wide.reset_index(), id_vars=[\"index\", \"name\"], var_name=\"variable\", value_name=\"value\")\n\nconstituency_actuals.index = constituencies.code.values\nconstituency_actuals[\"name\"] = constituencies.name.values\nconstituency_actuals_long = pd.melt(constituency_actuals.reset_index(), id_vars=[\"index\", \"name\"], var_name=\"variable\", value_name=\"value\")\n\nconstituency_target_validation = pd.merge(constituency_results, constituency_actuals_long, on=[\"index\", \"variable\"], suffixes=(\"_target\", \"_actual\"))\nconstituency_target_validation.drop(\"name_actual\", axis=1, inplace=True)\nconstituency_target_validation.columns = [\"index\", \"name\", \"metric\", \"estimate\", \"target\"]\n\nconstituency_target_validation[\"error\"] = constituency_target_validation[\"estimate\"] - constituency_target_validation[\"target\"]\nconstituency_target_validation[\"abs_error\"] = constituency_target_validation[\"error\"].abs()\nconstituency_target_validation[\"rel_abs_error\"] = constituency_target_validation[\"abs_error\"] / constituency_target_validation[\"target\"]\n\n","type":"content","url":"/validation/constituencies#constituency-dataset-validation","position":3},{"hierarchy":{"lvl1":"Constituency dataset validation","lvl2":"Calibration check"},"type":"lvl2","url":"/validation/constituencies#calibration-check","position":4},{"hierarchy":{"lvl1":"Constituency dataset validation","lvl2":"Calibration check"},"content":"Looking at the sorted validation results by relative absolute error shows how well our calibrated weights perform against the actual target statistics across UK parliamentary constituencies under the new 2024 boundaries. The table reveals the accuracy of our estimates, from the closest matches to the largest discrepancies, where a lower relative error indicates better calibration performance.\n\nconstituency_target_validation.sort_values(\"rel_abs_error\", ascending=False)\n\nnational_performance = household_weights @ national_target_matrix\nnational_target_validation = pd.DataFrame({\"metric\": national_performance.index, \"estimate\": national_performance.values})\nnational_target_validation[\"target\"] = national_actuals.values\n\nnational_target_validation[\"error\"] = national_target_validation[\"estimate\"] - national_target_validation[\"target\"]\nnational_target_validation[\"abs_error\"] = national_target_validation[\"error\"].abs()\nnational_target_validation[\"rel_abs_error\"] = national_target_validation[\"abs_error\"] / national_target_validation[\"target\"]\n\nThe table below shows the relative absolute error for each calibration target at the national level, sorted from the closest matches to the largest discrepancies.\n\nnational_target_validation.sort_values(\"rel_abs_error\")","type":"content","url":"/validation/constituencies#calibration-check","position":5},{"hierarchy":{"lvl1":"Validation"},"type":"lvl1","url":"/validation","position":0},{"hierarchy":{"lvl1":"Validation"},"content":"The pages in this section show how PolicyEngine’s data compares to external statistics.","type":"content","url":"/validation","position":1},{"hierarchy":{"lvl1":"Local authority dataset validation"},"type":"lvl1","url":"/validation/local-authorities","position":0},{"hierarchy":{"lvl1":"Local authority dataset validation"},"content":"","type":"content","url":"/validation/local-authorities","position":1},{"hierarchy":{"lvl1":"Local authority dataset validation"},"type":"lvl1","url":"/validation/local-authorities#local-authority-dataset-validation","position":2},{"hierarchy":{"lvl1":"Local authority dataset validation"},"content":"\n\nfrom policyengine_uk import Microsimulation\nimport pandas as pd\nimport h5py\nimport numpy as np\nimport sys\nfrom itables import init_notebook_mode\nimport itables.options as opt\nfrom pathlib import Path\nfrom policyengine_uk_data.storage import STORAGE_FOLDER\nfrom policyengine_uk_data.utils.huggingface import download\n\nopt.maxBytes = \"1MB\"\ninit_notebook_mode(all_interactive=True)\n\nREPO = Path(\".\").resolve().parent\n\nweights_file_path = STORAGE_FOLDER / \"local_authority_weights.h5\"\nconstituency_names_file_path = download(\n    repo=\"policyengine/policyengine-uk-data\",\n    repo_filename=\"local_authorities_2021.csv\",\n    local_folder=None,\n    version=None,\n)\nconstituencies_2024 = pd.read_csv(constituency_names_file_path)\n\nwith h5py.File(weights_file_path, \"r\") as f:\n    weights = f[str(2025)][...]\n\nbaseline = Microsimulation()\nhousehold_weights = baseline.calculate(\"household_weight\", 2025).values\n\nfrom policyengine_uk_data.datasets.frs.local_areas.local_authorities.loss import create_local_authority_target_matrix, create_national_target_matrix\nfrom policyengine_uk_data.datasets import EnhancedFRS_2022_23\n\nlocal_authority_target_matrix, local_authority_actuals, _ = create_local_authority_target_matrix(EnhancedFRS_2022_23, 2025, None)\nnational_target_matrix, national_actuals = create_national_target_matrix(EnhancedFRS_2022_23, 2025, None)\n\nlocal_authority_wide = weights @ local_authority_target_matrix\nlocal_authority_wide.index = constituencies_2024.code.values\nlocal_authority_wide[\"name\"] = constituencies_2024.name.values\n\nlocal_authority_results = pd.melt(local_authority_wide.reset_index(), id_vars=[\"index\", \"name\"], var_name=\"variable\", value_name=\"value\")\n\nlocal_authority_actuals.index = constituencies_2024.code.values\nlocal_authority_actuals[\"name\"] = constituencies_2024.name.values\nlocal_authority_actuals_long = pd.melt(local_authority_actuals.reset_index(), id_vars=[\"index\", \"name\"], var_name=\"variable\", value_name=\"value\")\n\nlocal_authority_target_validation = pd.merge(local_authority_results, local_authority_actuals_long, on=[\"index\", \"variable\"], suffixes=(\"_target\", \"_actual\"))\nlocal_authority_target_validation.drop(\"name_actual\", axis=1, inplace=True)\nlocal_authority_target_validation.columns = [\"index\", \"name\", \"metric\", \"estimate\", \"target\"]\n\nlocal_authority_target_validation[\"error\"] = local_authority_target_validation[\"estimate\"] - local_authority_target_validation[\"target\"]\nlocal_authority_target_validation[\"abs_error\"] = local_authority_target_validation[\"error\"].abs()\nlocal_authority_target_validation[\"rel_abs_error\"] = local_authority_target_validation[\"abs_error\"] / local_authority_target_validation[\"target\"]\n\n","type":"content","url":"/validation/local-authorities#local-authority-dataset-validation","position":3},{"hierarchy":{"lvl1":"Local authority dataset validation","lvl2":"Calibration check"},"type":"lvl2","url":"/validation/local-authorities#calibration-check","position":4},{"hierarchy":{"lvl1":"Local authority dataset validation","lvl2":"Calibration check"},"content":"Looking at the sorted validation results by relative absolute error shows how well our calibrated weights perform against the actual target statistics across UK local authorities under 2021 boundaries. The table reveals the accuracy of our estimates, from the closest matches to the largest discrepancies, where a lower relative error indicates better calibration performance.\n\nlocal_authority_target_validation.sort_values(\"rel_abs_error\")\n\nnational_performance = household_weights @ national_target_matrix\nnational_target_validation = pd.DataFrame({\"metric\": national_performance.index, \"estimate\": national_performance.values})\nnational_target_validation[\"target\"] = national_actuals.values\n\nnational_target_validation[\"error\"] = national_target_validation[\"estimate\"] - national_target_validation[\"target\"]\nnational_target_validation[\"abs_error\"] = national_target_validation[\"error\"].abs()\nnational_target_validation[\"rel_abs_error\"] = national_target_validation[\"abs_error\"] / national_target_validation[\"target\"]\n\nThe table below shows the relative absolute error for each calibration target at the national level, sorted from the closest matches to the largest discrepancies.\n\nnational_target_validation.sort_values(\"rel_abs_error\")","type":"content","url":"/validation/local-authorities#calibration-check","position":5},{"hierarchy":{"lvl1":"National dataset validation"},"type":"lvl1","url":"/validation/national","position":0},{"hierarchy":{"lvl1":"National dataset validation"},"content":"from policyengine_uk_data import EnhancedFRS_2022_23, FRS_2022_23, SPI_2020_21\nfrom policyengine_uk_data.utils.loss import get_loss_results\nimport pandas as pd\nfrom itables import init_notebook_mode\nimport itables.options as opt\nopt.maxBytes = \"1MB\"\n\ninit_notebook_mode(all_interactive=True)\n\ndef get_validation():\n    df = pd.DataFrame()\n    for dataset in [FRS_2022_23, EnhancedFRS_2022_23]:\n        for year in range(2025, 2029):\n            loss_results = get_loss_results(dataset, year)\n            loss_results[\"time_period\"] = year\n            loss_results[\"dataset\"] = dataset.label\n            df = pd.concat([df, loss_results])\n    df = df.reset_index(drop=True)\n    return df\n\ndf = get_validation()\ntruth_df = df[df.dataset == df.dataset.unique()[0]].reset_index()\ntruth_df[\"estimate\"] = truth_df[\"target\"]\ntruth_df[\"error\"] = truth_df[\"estimate\"] - truth_df[\"target\"]\ntruth_df[\"abs_error\"] = truth_df[\"error\"].abs()\ntruth_df[\"rel_error\"] = truth_df[\"error\"] / truth_df[\"target\"]\ntruth_df[\"abs_rel_error\"] = truth_df[\"rel_error\"].abs()\ntruth_df[\"dataset\"] = \"Official\"\ndf = pd.concat([df, truth_df]).reset_index(drop=True)\n\nCalibration check: the table below shows how both the original and enhanced FRS datasets compare to over 2,000 official statistics (which the EFRS was explicitly calibrated to hit) from the OBR, DWP and HMRC.\n\nSince the EFRS is calibrated to these statistics, high performance is expected and achieved.","type":"content","url":"/validation/national","position":1},{"hierarchy":{"lvl1":"National dataset validation","lvl2":"Full results"},"type":"lvl2","url":"/validation/national#full-results","position":2},{"hierarchy":{"lvl1":"National dataset validation","lvl2":"Full results"},"content":"\n\ndf.drop(columns=[\"index\"])\n\n","type":"content","url":"/validation/national#full-results","position":3},{"hierarchy":{"lvl1":"National dataset validation","lvl2":"Comparisons"},"type":"lvl2","url":"/validation/national#comparisons","position":4},{"hierarchy":{"lvl1":"National dataset validation","lvl2":"Comparisons"},"content":"\n\nmerged = pd.merge(\n    df[df.dataset == \"FRS (2022-23)\"],\n    df[df.dataset == \"Enhanced FRS (2022-23)\"],\n    on=[\"time_period\", \"name\"],\n    suffixes=(\"_frs\", \"_efrs\"),\n)\nmerged[\"rel_error_change_under_efrs\"] = merged[\"abs_rel_error_efrs\"] - merged[\"abs_rel_error_frs\"]\n# Sort columns\nmerged = merged[\n    [\n        \"name\",\n        \"time_period\",\n        \"target_frs\",\n        \"estimate_frs\",\n        \"estimate_efrs\",\n        \"error_frs\",\n        \"error_efrs\",\n        \"abs_error_frs\",\n        \"abs_error_efrs\",\n        \"rel_error_frs\",\n        \"rel_error_efrs\",\n        \"abs_rel_error_frs\",\n        \"abs_rel_error_efrs\",\n        \"rel_error_change_under_efrs\",\n    ]\n]\nmerged","type":"content","url":"/validation/national#comparisons","position":5}]}